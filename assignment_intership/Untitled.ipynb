{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34768092-0089-4bb5-8b65-28ecf0f2abc2",
   "metadata": {},
   "source": [
    " ## Algorithm1: Calculating Target Variables\n",
    "1. Assign values to variables\n",
    "\n",
    "a = 1-1 semester percentage, \n",
    " b= 1-2 semester percentage\n",
    "\n",
    "c= 2-1 semester percentage,\n",
    " d = 2-2 semester percentage\n",
    "\n",
    "e= 3-1 semester percentage,\n",
    " f = 3-2 semester Percentage\n",
    " \n",
    "g = Attendance percentage,\n",
    " h = extracurricular activities\n",
    "\n",
    "i = Academic awards and achievements,\n",
    " j = Coding skills\n",
    "\n",
    "k = semester_grades=[a,b,c,d,e,f]\n",
    "\n",
    "2. Calculate dropout\n",
    "dropout = 1 if min(k) < 35 and g < 30 else 0\n",
    "\n",
    "3. Calculate good performance\n",
    "good_performance = 1 if all(grade > 60 for grade in k) else 0\n",
    "\n",
    "4. Calculate poor performance\n",
    "poor_performance = 1 if max(k) < 40 else 0\n",
    "\n",
    "5. Calculate support required\n",
    "support_required = 1 if any(40 <= grade < 60 for grade in k) else 0\n",
    "\n",
    "6. Calculate eligibility for placement\n",
    "eligible_for_placement = 1 if all(grade > 65 for grade in k) and (j or i or h) else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5171396-28c1-480b-9fce-76ef0112d97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 0\n",
      "good_performance: 1\n",
      "poor_performance: 0\n",
      "support_required: 0\n",
      "eligible_for_placement: 1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Assign values to variables\n",
    "a = 80 # we can take input also from user using a = int(input(\"message to display\"))\n",
    "b = 75\n",
    "c = 85\n",
    "d = 90\n",
    "e = 70\n",
    "f = 80\n",
    "g = 75\n",
    "h = True\n",
    "i = False\n",
    "j = True\n",
    "semester_grades = [a, b, c, d, e, f]\n",
    "\n",
    "# Step 2: Calculate dropout\n",
    "dropout = 1 if min(semester_grades) < 35 and g < 30 else 0\n",
    "\n",
    "# Step 3: Calculate good performance\n",
    "good_performance = 1 if all(grade > 60 for grade in semester_grades) else 0\n",
    "\n",
    "# Step 4: Calculate poor performance\n",
    "poor_performance = 1 if max(semester_grades) < 40 else 0\n",
    "\n",
    "# Step 5: Calculate support required\n",
    "support_required = 1 if any(40 <= grade < 60 for grade in semester_grades) else 0\n",
    "\n",
    "# Step 6: Calculate eligibility for placement\n",
    "eligible_for_placement = 1 if all(grade > 65 for grade in semester_grades) and (j or i or h) else 0\n",
    "\n",
    "# Print the calculated variables\n",
    "print(\"dropout:\", dropout)\n",
    "print(\"good_performance:\", good_performance)\n",
    "print(\"poor_performance:\", poor_performance)\n",
    "print(\"support_required:\", support_required)\n",
    "print(\"eligible_for_placement:\", eligible_for_placement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29774ba4-7e80-4c43-8723-f1611860da17",
   "metadata": {},
   "source": [
    "## Algorithm 2: LSTM for Student Academic Performance Evaluation:\n",
    "Description: This algorithm analyzes student performance using an LSTM model. The input data includes \n",
    "student academic performance metrics, and the output is a prediction of good performers, poor \n",
    "performers, students who require support and the dropouts\n",
    "\n",
    "Input: semester percentages\n",
    "\n",
    "Output: A binary classification indicating whether a student is a good performer or bad performer or \n",
    "requiring support or dropout\n",
    "\n",
    "Procedure:\n",
    "\n",
    "1. Import required libraries for data analysis, data cleaning, visualization, and LSTM modeling.\n",
    "2. Load the dataset of student academic performance metrics \n",
    "3. Clean the data \n",
    "4. Evaluate the academic performance of good performers, poor performers, student who require  support, student dropouts, students with placement eligibility by calculating their cumulative  percentage and display the output.\n",
    "5. Visualize the critical values as graphs across all students \n",
    "6. Visualize the critical values as graphs across all students\n",
    "7. Prepare the data for the LSTM model by separating input features and the target variable.\n",
    "8. Split the data into training and testing sets \n",
    "9. Reshape the input data into the required format for LSTM modeling.\n",
    "10. Build and compile the LSTM model \n",
    "11. Train the LSTM model on the training data.\n",
    "12. Evaluate the LSTM model and print accuracies of trained LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2607a0b0-b08c-4072-9bb5-969acf3af9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "Successfully installed keras-2.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fdc6a6-25eb-4517-9046-d65285cc5401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.8.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting ml-dtypes>=0.0.3\n",
      "  Downloading ml_dtypes-0.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.9.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439678 sha256=ad32d6e6c96e8784f3dbaa58dcbb1c795cf41694b876039120fbb2f0bd1f35a9\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/d6/f6/d5/63686989c723075de411cbc630ca12f4241a8436e411e38d6a\n",
      "Successfully built jax\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 jax-0.4.8 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.2 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.2.0 werkzeug-2.2.3 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a81517-c5a4-4fe8-a2f1-34f1f02dd849",
   "metadata": {},
   "source": [
    "### Note\n",
    "that this code assumes that the dataset is stored in a CSV file called \"student_performance.csv\". You can replace this filename with your own dataset filename. The code also assumes that the input data has seven features (semester percentages), and the output is a binary classification indicating whether a student is a good performer or bad performer or requiring support or dropout. If your dataset has a different number of features or a different output classification, you will need to modify the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab35b7f-0937-49c0-9816-9581d26eaeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "dataset = pd.read_csv(\"student_performance.csv\")\n",
    "\n",
    "# Step 3: Clean the data (if necessary)\n",
    "\n",
    "# Step 4: Evaluate academic performance\n",
    "good_performance = dataset[(dataset['cumulative_percentage'] >= 60)]\n",
    "poor_performance = dataset[(dataset['cumulative_percentage'] < 40)]\n",
    "support_required = dataset[(dataset['cumulative_percentage'] >= 40) & (dataset['cumulative_percentage'] < 60)]\n",
    "dropout = dataset[(dataset['cumulative_percentage'] < 35) & (dataset['attendance_percentage'] < 30)]\n",
    "eligible_for_placement = dataset[(dataset['cumulative_percentage'] >= 65) & (dataset['coding_skills'] | dataset['academic_awards'] | dataset['extracurricular_activities'])]\n",
    "\n",
    "# Display the output\n",
    "print(\"Good performers:\", len(good_performance))\n",
    "print(\"Poor performers:\", len(poor_performance))\n",
    "print(\"Support required:\", len(support_required))\n",
    "print(\"Dropouts:\", len(dropout))\n",
    "print(\"Eligible for placement:\", len(eligible_for_placement))\n",
    "\n",
    "# Step 5: Visualize the critical values\n",
    "# Code for visualization goes here\n",
    "\n",
    "# Step 6: Visualize the critical values\n",
    "# Code for visualization goes here\n",
    "\n",
    "# Step 7: Prepare the data for LSTM model\n",
    "X = dataset.iloc[:, 1:8].values\n",
    "y = dataset.iloc[:, 8].values\n",
    "\n",
    "# Step 8: Split the data into training and testing sets\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_X, test_X = X[0:train_size,:], X[train_size:len(dataset),:]\n",
    "train_y, test_y = y[0:train_size], y[train_size:len(dataset)]\n",
    "\n",
    "# Step 9: Reshape the input data\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "# Step 10: Build and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(1, 7)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 11: Train the LSTM model\n",
    "model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y))\n",
    "\n",
    "# Step 12: Evaluate the LSTM model\n",
    "scores = model.evaluate(test_X, test_y, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee7a20-1728-4625-af33-ffe488efdf00",
   "metadata": {},
   "source": [
    "## If you have any query regarding this plz free to ask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d709eb-0368-41ac-aca3-3ecfc9bf0c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
